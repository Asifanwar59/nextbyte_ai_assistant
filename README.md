# Nextbyte AI Assistant

## Introduction 
Nextbyte AI Assistant is an enterprise-grade agent based RAG solution that enables users to query local PDF repositories using Large Language Models. Built with a unified architecture, it integrates robust safety guardrails and advanced orchestration/workflow to ensure reliable, context-aware interactions.

## Key Features
Local Data Sync: Automated scanning and indexing of PDFs from project-root/data/uploads, removing the need for manual browser-based uploads.
Unified Full-Stack Architecture: A seamless integration of a FastAPI backend and a React/Tailwind CSS frontend for low-latency communication to use single docker build for both fastapi backend and frontend.
RAG-Powered Conversations: Contextual answering using vector embeddings to retrieve relevant document segments.

Real-time Sync: Background processing of documents to keep the AI's knowledge base updated without interrupting the user experience.
Dockerized Environment: Fully containerized setup for consistent deployment across development and production environments.

## Architectural Advantages

### Advanced Guardrails
Safety & Compliance: Implements input/output filtering to prevent the disclosure of PII (Personally Identifiable Information) and mitigate "jailbreaking" attempts.
Hallucination Control: Restricts the LLM to provide answers based only on the provided documents, reducing the risk of misinformation.

### LangGraph Orchestration
Stateful Multi-Agent Flows: Uses LangGraph to manage complex conversation states, allowing the system to handle multi-turn reasoning and iterative retrieval.
Cyclic Logic: Enables the assistant to "self-correct" by re-searching if the initial retrieval does not adequately answer the user's query.

### Unified Backend & Frontend
Single Source of Truth: Shared environment variables and API schemas reduce integration errors. Single docker build for both fastapi backend and frontend.

Developer Velocity: Faster iterations by managing the entire application lifecycle (from data ingestion to UI display) within a single repository.

## Scalability & AWS Extension

### Docker: 
The system is designed with a "Cloud-Ready" mindset, making it easily extendable to Amazon Web Services (AWS)
Compute: Transition from local Docker to AWS ECS (Elastic Container Service).

### Database: 
Replace local vector storage with Amazon OpenSearch or Pinecone for handling millions of documents.

### Storage: 
Swap the local data/uploads folder for an S3 Bucket with S3 Event Notifications to trigger indexing.

### API: 
Use AWS App Runner or Lambda to scale the backend based on demand.

## Further Scope for Optimization
### Hybrid Search
Combine semantic vector search with keyword-based (BM25) search for higher retrieval accuracy on technical jargon.

### Streaming Responses: 
Implement Server-Sent Events (SSE) or WebSockets to stream LLM responses word-by-word, improving perceived latency.

### Authentication & RBAC:
Implement OAuth2 (Cognito/Auth0) to provide Role-Based Access Control, ensuring users only query documents they are authorized to see.

### Multi-Modal Ingestion: 
Expand processing to include Excel, Word, and Powerpoint files using libraries like unstructured or text image based pdf files using openAI CLIP embedding midel.

### Observability: 
Integrate LangSmith or AWS CloudWatch to track LLM costs, latency, and retrieval quality (faithfulness/relevance).

## Quick Start
Place PDFs in nextbyte_ai_assistant/data/uploads.

Build the image: 
''' 
docker build -t rag-unified .
'''
Run the container:
bash
''' docker run -p 8000:8000 -v ${PWD}/data/uploads:/app/data/uploads --env-file .env rag-unified
'''

Click "Sync Local Data" on the dashboard to fetch latest files.

